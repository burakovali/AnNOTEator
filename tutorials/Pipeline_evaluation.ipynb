{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bb6b16",
   "metadata": {},
   "source": [
    "# Pipeline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d9e26",
   "metadata": {},
   "source": [
    "We noticed that the output from the drum_exraction function has minor differences from time to time, which also impacted the dataframe output of the drum_to_frame function. It is impossible to provide manually created label for every possible outcome from the drum_to_frame function. Therefore, we decided to pre-process 2 songs, convert it into dataframe using the drum_to_frame function and manually transcribe it for evaluation.\n",
    "\n",
    "To avoid potential copyright issues, we also hide the song name and labeled it as song_1 and song_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To begin evaluation, we first need to load the dataframe from pickles\n",
    "import pandas as pd\n",
    "df_song_1=pd.read_pickle('../model_development/song_1.pkl')\n",
    "df_song_2=pd.read_pickle('../model_development/song_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7343c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the pre-trained model\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('../inference/pretrained_models/annoteators/complete_network.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b6e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function here to convert the df.audio_clip into mel-frequency spectrogram, and make the prediction\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(df, song_sampling_rate):\n",
    "    df=df.copy()\n",
    "    pred_x = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        pred_x.append(librosa.feature.melspectrogram(y=df.audio_clip.iloc[i], \n",
    "                                                 sr=song_sampling_rate, n_mels=128, fmax=8000))\n",
    "        \n",
    "    X = np.array(pred_x)\n",
    "    X = X.reshape(X.shape[0],X.shape[1],X.shape[2],1)\n",
    "    result = []\n",
    "    pred_raw=model.predict(X)\n",
    "    \n",
    "    pred = np.round(pred_raw)\n",
    "\n",
    "    for i in range(pred_raw.shape[0]):\n",
    "        prediction = pred[i]\n",
    "        if sum(prediction) == 0:\n",
    "            raw = pred_raw[i]\n",
    "            new = np.zeros(6)\n",
    "            ind = raw.argmax()\n",
    "            new[ind] = 1\n",
    "            result.append(new)\n",
    "        else:\n",
    "            result.append(prediction)\n",
    "\n",
    "    result = np.array(result)\n",
    "\n",
    "    drum_hits = ['SD','HH','KD','RC','TT','CC']\n",
    "    prediction = pd.DataFrame(result, columns = drum_hits)\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    prediction.reset_index(inplace=True)\n",
    "\n",
    "    result = df.merge(prediction,left_on='index', right_on= 'index')\n",
    "    result.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14765dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute exact and partial match score \n",
    "#\"Exact match” means the prediction needs to be exactly matched with all the true labels regardless of whether\n",
    "# they are multi-labeled or single-labeled. “\n",
    "# Partial match” means the prediction only needs to be matched with one of the true labels.\n",
    "\n",
    "def customized_metric(df):\n",
    "    df=df.copy()\n",
    "    for pair in list(zip(['SD_T','HH_T','KD_T','RC_T','TT_T','CC_T'], ['SD','HH','KD','RC','TT','CC'])):\n",
    "        df[pair[0]].replace(1, pair[1], inplace=True)\n",
    "        df[pair[1]].replace(1, pair[1], inplace=True)\n",
    "    def create_truth_set(x):\n",
    "        s=set([x['SD_T'], x['HH_T'],x['KD_T'],x['RC_T'],x['TT_T'],x['CC_T']])\n",
    "        s.remove(0)\n",
    "        return s\n",
    "\n",
    "    def create_pred_set(x):\n",
    "        s=set([x['SD'], x['HH'],x['KD'],x['RC'],x['TT'],x['CC']])\n",
    "        s.remove(0)\n",
    "        return s\n",
    "\n",
    "    df['true']=df.apply(lambda x:create_truth_set(x), axis=1)\n",
    "    df['pred']=df.apply(lambda x:create_pred_set(x), axis=1)\n",
    "    df['exact']=df['true']==df['pred']\n",
    "\n",
    "    def intersect(x):\n",
    "        if len(x['true'].intersection(x['pred']))>0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    df['partial']=df.apply(lambda x:intersect(x), axis=1)\n",
    "\n",
    "    print(f\"exact match: {round(df['exact'].value_counts(normalize=True)[True],2)}\")\n",
    "    print(f\"partial match: {round(df['partial'].value_counts(normalize=True)[True],2)}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333e03c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song_1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          SD       0.50      0.49      0.50        59\n",
      "          HH       0.90      0.60      0.72       316\n",
      "          KD       0.94      0.74      0.83       164\n",
      "          RC       0.00      0.00      0.00         1\n",
      "          TT       0.55      0.60      0.57        40\n",
      "          CC       0.00      0.00      0.00         3\n",
      "\n",
      "   micro avg       0.82      0.63      0.71       583\n",
      "   macro avg       0.48      0.41      0.44       583\n",
      "weighted avg       0.84      0.63      0.71       583\n",
      " samples avg       0.82      0.68      0.72       583\n",
      "\n",
      "song_1\n",
      "exact match: 0.52\n",
      "partial match: 0.83\n"
     ]
    }
   ],
   "source": [
    "#Then we will use the classification_report function to evaluate the result\n",
    "from sklearn.metrics import classification_report\n",
    "pred_df=evaluate(df_song_1, 44100)\n",
    "\n",
    "labels=['SD','HH','KD','RC','TT','CC']\n",
    "print('song_1')\n",
    "print(classification_report(pred_df[['SD_T','HH_T','KD_T','RC_T','TT_T','CC_T']],\n",
    "                            pred_df[['SD','HH','KD','RC','TT','CC']],\n",
    "                           target_names=labels,                            \n",
    "                           zero_division=0))\n",
    "\n",
    "print('song_1')\n",
    "customized_metric(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cd68f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          SD       0.88      0.69      0.77       212\n",
      "          HH       0.61      0.14      0.23       241\n",
      "          KD       0.91      0.89      0.90       313\n",
      "          RC       0.00      0.00      0.00         0\n",
      "          TT       0.07      0.29      0.11        14\n",
      "          CC       0.00      0.00      0.00       233\n",
      "\n",
      "   micro avg       0.79      0.46      0.58      1013\n",
      "   macro avg       0.41      0.33      0.34      1013\n",
      "weighted avg       0.61      0.46      0.50      1013\n",
      " samples avg       0.80      0.47      0.58      1013\n",
      "\n",
      "song_2\n",
      "exact match: 0.13\n",
      "partial match: 0.81\n"
     ]
    }
   ],
   "source": [
    "#Then we will use the precision_recall_fscore_support function to evaluate the result\n",
    "\n",
    "pred_df=evaluate(df_song_2, 44100)\n",
    "\n",
    "labels=['SD','HH','KD','RC','TT','CC']\n",
    "print('song_2')\n",
    "print(classification_report(pred_df[['SD_T','HH_T','KD_T','RC_T','TT_T','CC_T']],\n",
    "                            pred_df[['SD','HH','KD','RC','TT','CC']],\n",
    "                           target_names=labels,\n",
    "                           zero_division=0))\n",
    "\n",
    "print('song_2')\n",
    "customized_metric(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a3aec5d93e013113e0745b9743bfdf778cff92e1ca9a8a8c7c8cbf095dc8ebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
